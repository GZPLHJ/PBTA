# Ultralytics YOLO ğŸš€, GPL-3.0 license
"""
Common modules
"""

import math

import torch
import torch.nn as nn

from ultralytics.yolo.utils.tal import dist2bbox, make_anchors


import torch
from torch import nn
from torch.nn import functional as F
from torch.autograd import Function, Variable
from torch.nn import Module, parameter

from queue import Queue
import warnings
# try:
#     from queue import Queue
# except ImportError:
    # from Queue import Queue

from torch.nn.modules.batchnorm import _BatchNorm
from functools import partial


from timm.models.layers import DropPath, trunc_normal_



def autopad(k, p=None, d=1):  # kernel, padding, dilation
    # Pad to 'same' shape outputs
    if d > 1:
        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size
    if p is None:
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad
    return p




#  1*1 3*3 1*1
class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1, res_conv=False, act_layer=nn.ReLU, groups=1,
                 norm_layer=partial(nn.BatchNorm2d, eps=1e-6), drop_block=None, drop_path=None):
        super(ConvBlock, self).__init__()
        self.in_channels = in_channels
        expansion = 4
        c = out_channels // expansion

        self.conv1 = nn.Conv2d(in_channels, c, kernel_size=1, stride=1, padding=0, bias=False)  # [64, 256, 1, 1]
        self.bn1 = norm_layer(c)
        self.act1 = act_layer(inplace=True)

        self.conv2 = nn.Conv2d(c, c, kernel_size=3, stride=stride, groups=groups, padding=1, bias=False)
        self.bn2 = norm_layer(c)
        self.act2 = act_layer(inplace=True)

        self.conv3 = nn.Conv2d(c, out_channels, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn3 = norm_layer(out_channels)
        self.act3 = act_layer(inplace=True)

        if res_conv:
            self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)
            self.residual_bn = norm_layer(out_channels)

        self.res_conv = res_conv
        self.drop_block = drop_block
        self.drop_path = drop_path

    def zero_init_last_bn(self):
        nn.init.zeros_(self.bn3.weight)

    def forward(self, x, return_x_2=False):#é»˜è®¤å€¼æ˜¯return_x_2=Trueã€‚è‹¥ä¸ºTrueé‚£è¾“å‡ºçš„æ˜¯ä¸¤ä¸ªå€¼ã€‚è¾“å…¥ï¼š[6, 32, 640, 640]ï¼Œè¾“å‡ºï¼šåˆ†åˆ«æ˜¯[6, 16, 640, 640]ï¼Œ[6, 64, 640, 640]
        residual = x

        x = self.conv1(x)
        x = self.bn1(x)
        if self.drop_block is not None:
            x = self.drop_block(x)
        x = self.act1(x)

        x = self.conv2(x) #if x_t_r is None else self.conv2(x + x_t_r)
        x = self.bn2(x)
        if self.drop_block is not None:
            x = self.drop_block(x)
        x2 = self.act2(x)

        x = self.conv3(x2)
        x = self.bn3(x)
        if self.drop_block is not None:
            x = self.drop_block(x)

        if self.drop_path is not None:
            x = self.drop_path(x)

        if self.res_conv:
            residual = self.residual_conv(residual)
            residual = self.residual_bn(residual)

        x += residual
        x = self.act3(x)

        if return_x_2:
            return x, x2
        else:
            return x


class Mean(nn.Module):
    def __init__(self, dim, keep_dim=False):
        super(Mean, self).__init__()
        self.dim = dim
        self.keep_dim = keep_dim

    def forward(self, input):
        return input.mean(self.dim, self.keep_dim)


class Mlp(nn.Module):
    """
    Implementation of MLP with 1*1 convolutions. Input: tensor with shape [B, C, H, W]
    """
    def __init__(self, in_features, hidden_features=None,
                 out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Conv2d(in_features, hidden_features, 1)
        self.act = act_layer()
        self.fc2 = nn.Conv2d(hidden_features, out_features, 1)
        self.drop = nn.Dropout(drop)
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Conv2d):
            trunc_normal_(m.weight, std=.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class LayerNormChannel(nn.Module):
    """
    LayerNorm only for Channel Dimension.
    Input: tensor in shape [B, C, H, W]
    """
    def __init__(self, num_channels, eps=1e-05):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(num_channels))
        self.bias = nn.Parameter(torch.zeros(num_channels))
        self.eps = eps

    def forward(self, x):
        u = x.mean(1, keepdim=True)
        s = (x - u).pow(2).mean(1, keepdim=True)
        x = (x - u) / torch.sqrt(s + self.eps)
        x = self.weight.unsqueeze(-1).unsqueeze(-1) * x \
            + self.bias.unsqueeze(-1).unsqueeze(-1)
        return x


class GroupNorm(nn.GroupNorm):
    """
    Group Normalization with 1 group.
    Input: tensor in shape [B, C, H, W]
    """
    def __init__(self, num_channels, **kwargs):
        super().__init__(1, num_channels, **kwargs)




# region LVCBlock
class LVCBlock(nn.Module):
    def __init__(self, c1, c2, num_codes, channel_ratio=0.25, base_channel=64):
        super(LVCBlock, self).__init__()
        self.c2 = c2
        self.num_codes = num_codes
        num_codes = 64

        self.conv_1 = ConvBlock(in_channels=c1, out_channels=c2, res_conv=True, stride=1)
        #return_x_2=Trueæ—¶ è¾“å…¥ï¼š[6, 32, 640, 640]ï¼Œè¾“å‡ºï¼šåˆ†åˆ«æ˜¯[6, 16, 640, 640]ï¼Œ[6, 64, 640, 640]
        #return_x_2=Falseæ—¶ è¾“å…¥ï¼š[6, 32, 640, 640]ï¼Œè¾“å‡ºï¼šåˆ†åˆ«æ˜¯[6, 64, 640, 640]
        self.LVC = nn.Sequential(
            nn.Conv2d(c1, c1, kernel_size=1, bias=False),
            nn.BatchNorm2d(c1),
            nn.ReLU(inplace=True),
            Encoding(in_channels=c1, num_codes=num_codes),
            nn.BatchNorm1d(num_codes),
            nn.ReLU(inplace=True),
            Mean(dim=1))
        self.fc = nn.Sequential(nn.Linear(c1, c1), nn.Sigmoid())
    def forward(self, x):
        # x = self.conv_1(x, return_x_2=False)
        x = self.conv_1(x)
        en = self.LVC(x)
        gam = self.fc(en)
        b, in_channels, _, _ = x.size()
        y = gam.view(b, in_channels, 1, 1)
        x = F.relu(x + x * y,inplace=False)
        return x

# endregion

# region L-MLPBlock
# LightMLPBlock
class LightMLPBlock(nn.Module):
    '''
    x = x + self.drop_path(self.layer_scale_1.unsqueeze(-1).unsqueeze(-1) * self.dw(self.norm1(x)))æ³¨æ„MLPè¦æ±‚æ­¤ç‰¹å¾å›¾è¾“å…¥è¾“å‡ºå˜åŒ–å¾—ä¸€æ ·æ‰è¡Œï¼Œè¿™é‡Œ
    æ³¨æ„MLPè¦æ±‚æ­¤ç‰¹å¾å›¾è¾“å…¥è¾“å‡ºå˜åŒ–å¾—ä¸€æ ·æ‰è¡Œï¼Œè¿™é‡Œ
    '''
    def __init__(self, c1, c2, ksize=1, stride=1, act="silu",
                 mlp_ratio=4., drop=0., act_layer=nn.GELU,
                 use_layer_scale=True, layer_scale_init_value=1e-5, drop_path=0.,
                 norm_layer=GroupNorm):  # act_layer=nn.GELU,
        super().__init__()
        self.dw = DWConv_m_a(c1, c2, k=1, s=1, act="silu")
        self.linear = nn.Linear(c2, c2)  # learnable position embedding
        self.out_channels = c2

        self.norm1 = norm_layer(c1)
        self.norm2 = norm_layer(c1)

        mlp_hidden_dim = int(c1 * mlp_ratio)
        self.mlp = Mlp(in_features=c1, hidden_features=mlp_hidden_dim, act_layer=nn.GELU,
                       drop=drop)

        self.drop_path = DropPath(drop_path) if drop_path > 0. \
            else nn.Identity()

        self.use_layer_scale = use_layer_scale
        if use_layer_scale:
            self.layer_scale_1 = nn.Parameter(
                layer_scale_init_value * torch.ones((c2)), requires_grad=True)
            self.layer_scale_2 = nn.Parameter(
                layer_scale_init_value * torch.ones((c2)), requires_grad=True)

    def forward(self, x):
        if self.use_layer_scale:
            x = x + self.drop_path(self.layer_scale_1.unsqueeze(-1).unsqueeze(-1) * self.dw(self.norm1(x)))
            x = x + self.drop_path(self.layer_scale_2.unsqueeze(-1).unsqueeze(-1) * self.mlp(self.norm2(x)))
        else:
            x = x + self.drop_path(self.dw(self.norm1(x)))
            x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x


# endregion

# region EVC Block
# EVCBlock
class EVCBlock(nn.Module):
    def __init__(self, c1, c2, channel_ratio=4, base_channel=16):
        super().__init__()
        expansion = 2
        ch = c2 * expansion
        # Stem stage: get the feature maps by conv block (copied form resnet.py) è¿›å…¥conformeræ¡†æ¶ä¹‹å‰çš„å¤„ç†
        # !!!!!!æ­¤å¤„ç†è¾“å‡ºçš„å›¾ç‰‡é€šé“å°ºå¯¸å‡ä¸å˜
        self.conv1 = nn.Conv2d(c1, c1, kernel_size=7, stride=1, padding=3, bias=False)  # 1 / 2 [112, 112]
        self.bn1 = nn.BatchNorm2d(c1)
        self.act1 = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)  # 1 / 4 [56, 56]

        # LVC
        self.lvc = LVCBlock(c1=c1, c2=c2, num_codes=64)  # c1å€¼æš‚æ—¶æœªå®š
        # LightMLPBlock
        self.l_MLP = LightMLPBlock(c1, c2, ksize=1, stride=1, act="silu", act_layer=nn.GELU, mlp_ratio=4., drop=0.,
                                   use_layer_scale=True, layer_scale_init_value=1e-5, drop_path=0.,
                                   norm_layer=GroupNorm)
        self.cnv1 = nn.Conv2d(ch, c2, kernel_size=1, stride=1, padding=0)

    def forward(self, x):
        # é¢„å¤„ç† !!!!!!æ­¤å¤„ç†è¾“å‡ºçš„å›¾ç‰‡é€šé“å°ºå¯¸å‡ä¸å˜
        x1 = self.maxpool(self.act1(self.bn1(self.conv1(x))))
        # LVCBlock
        x_lvc = self.lvc(x1)
        # LightMLPBlock
        x_lmlp = self.l_MLP(x1)
        # concat concatå åŠ åç”±
        x = torch.cat((x_lvc, x_lmlp), dim=1)
        x = self.cnv1(x)
        return x
# endregion


#region 2.5 æ·±åº¦å¯åˆ†ç¦»å·ç§¯-actå¯é€‰æ‹©
def DWConv_m_a(c1, c2, k=1, s=1, act = "silu"):
    # Depthwise convolution
    return Conv_m_a(c1, c2, k, s, g=math.gcd(c1, c2), act=act)
#endregion

#region 3.5æ™®é€šå·ç§¯ conv+bn -actå¯é€‰æ‹©
class Conv_m_a(nn.Module):
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act="silu"):  # ch_in, ch_out, kernel, stride, padding, groups
        super(Conv_m_a, self).__init__()
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)
        self.bn = nn.BatchNorm2d(c2)
        self.act = get_activation(act, inplace=True)

    def forward(self, x):
        return self.act(self.bn(self.conv(x)))

    def fuseforward(self, x):
        return self.act(self.conv(x))
"""ç”¨äºModelç±»çš„fuseå‡½æ•°
èåˆconv+bn åŠ é€Ÿæ¨ç† ä¸€èˆ¬ç”¨äºæµ‹è¯•/éªŒè¯é˜¶æ®µ
"""

#endregion

#region actfunctioné€‰æ‹©
def get_activation(name="silu", inplace=True):
    if name == "silu":
        module = nn.SiLU(inplace=inplace)
    elif name == "relu":
        module = nn.ReLU(inplace=inplace)
    elif name == "lrelu":
        module = nn.LeakyReLU(0.1, inplace=inplace)
    else:
        raise AttributeError("Unsupported act type: {}".format(name))
    return module
#endregion


# LVC
class Encoding(nn.Module):
    def __init__(self, in_channels, num_codes):
        super(Encoding, self).__init__()
        # init codewords and smoothing factor
        self.in_channels, self.num_codes = in_channels, num_codes
        num_codes = 64
        std = 1. / ((num_codes * in_channels)**0.5)
        # [num_codes, channels] ç»è¿‡axivråˆå§‹åŒ–çš„äºŒç»´å¼ é‡
        self.codewords = nn.Parameter(
            torch.empty(num_codes, in_channels, dtype=torch.float).uniform_(-std, std), requires_grad=True)
        # [num_codes] ç»è¿‡axivråˆå§‹åŒ–çš„ä¸€ç»´å¼ é‡
        self.scale = nn.Parameter(torch.empty(num_codes, dtype=torch.float).uniform_(-1, 0), requires_grad=True)

    @staticmethod
    def scaled_l2(x, codewords, scale):##è¾“å…¥ xä¸º(32,50176,3)å’Œcodewordsï¼ˆnum_codes, in_channelsï¼‰æ„æˆçš„åˆå§‹åŒ–äºŒä½å¼ é‡
        num_codes, in_channels = codewords.size()
        b = x.size(0)
        expanded_x = x.unsqueeze(2).expand((b, x.size(1), num_codes, in_channels))#(32,50176,3)-ï¼ˆunsqueezeï¼‰->(32,50176,1,3)-(expand)->(32,50176,num_codes=64,3) å…¶ä¸­64*3
        # æ³¨æ„å…¶ä¸­expand()å‡½æ•°åœ¨ç¬¬ä¸‰ç»´åº¦è¿›è¡Œäº†æ‰©å±•ï¼Œæ‰©å±•çš„æ–¹å¼ï¼Œå°±æ˜¯æŠŠæœ€åçš„ç¬¬å››ç»´åº¦çš„é‚£3ä¸ªæ•°è¿›è¡Œå¤åˆ¶ï¼Œå¤åˆ¶64ï¼ˆnum_codesï¼‰æ¬¡

        # ---å¤„ç†codebook (num_code, c1)
        reshaped_codewords = codewords.view((1, 1, num_codes, in_channels))#(64,3)-(view)->(1,1,64,3)

        # æŠŠscaleä»1, num_codeå˜æˆ   batch, c2, N, num_codes
        reshaped_scale = scale.view((1, 1, num_codes))  # (num_codes=64)-(view)->(1,1,64)

        # ---è®¡ç®—rik = z1 - d  # b, N, num_codes
        scaled_l2_norm = reshaped_scale * (expanded_x - reshaped_codewords).pow(2).sum(dim=3)
        #[(32,50176,64,3)-(1,1,64,3)]->(32,50176,64,3)æ¯ä¸€ç»„ï¼ˆ3ä¸ªåƒç´ ç‚¹ç»„æˆçš„ç»„åˆï¼‰åƒç´ ç‚¹ä¸ç æœ¬ç›¸å‡
        #(32,50176,64,3)-ï¼ˆpow(2)ï¼‰-ã€‹(32,50176,64)
        #(32,50176,64)*(1,1,64)-ã€‹(32,50176,64)
        return scaled_l2_norm

    @staticmethod
    def aggregate(assignment_weights, x, codewords):#assignment_weights:(32,50176,64)  x:(32,3,224,224)   codewords:(num_codes=64, in_channels=3):(64,3)
        num_codes, in_channels = codewords.size()

        # ---å¤„ç†codebook
        reshaped_codewords = codewords.view((1, 1, num_codes, in_channels))#(64,3)->(1,1,64,3)
        b = x.size(0)

        # ---å¤„ç†ç‰¹å¾å‘é‡x b, c1, N
        expanded_x = x.unsqueeze(2).expand((b, x.size(1), num_codes, in_channels))#(32,50176,3)-ï¼ˆunsqueezeï¼‰->(32,50176,1,3)-(expand)->(32,50176,num_codes=64,3) å…¶ä¸­64*3

        #å˜æ¢rei  b, N, num_codes,-
        assignment_weights = assignment_weights.unsqueeze(3)  # b, N, num_codes,(32,50176,64)->(32,50176,64,1)

        # ---å¼€å§‹è®¡ç®—eik,å¿…é¡»åœ¨Reiè®¡ç®—å®Œä¹‹å
        encoded_feat = (assignment_weights * (expanded_x - reshaped_codewords)).sum(1)
        #{(32,50176,64,1)*[(32,50176,64,3)-(1,1,64,3)]}.sum(1)-----ã€‹{(32,50176,64,1)*(32,50176,64,3)}.sum(1)-----ã€‹(32,50176,64,3).sum(1)----ã€‹(32,64,3)
        #æ³¨æ„åœ¨æŸç»´åº¦æ±‚å’Œå°±æ˜¯å¯¹äºè¿™ä¸ªç»´åº¦çš„æ¯”å¦‚xä¸ªå—å½“ä¸­çš„åŒä¸€ä½ç½®å¤„æ±‚å’Œæœ€ç»ˆç”Ÿæˆä¸€ä¸ªå—ã€‚è€Œè¿™ä¸ªå—æ˜¯æ­¤ç»´åº¦ä¸‹å±ç»´åº¦çš„æ•°ç»„æˆçš„ã€‚æ¯”å¦‚(32,50176,64,3).sum(1)å°±æ˜¯åœ¨ç¬¬äºŒç»´åº¦è¿›è¡Œæ±‚å’Œï¼ŒæŠŠï¼ˆ64ï¼Œ3ï¼‰è¿™ä¸ªå—ä¹Ÿå°±æ˜¯ä¸‰å››ç»´çš„æ•°æ®ç»„æˆçš„å—ï¼Œåœ¨50176ä¸ªï¼ˆ64ï¼Œ3ï¼‰è¿™æ ·çš„å—çš„ç›¸åŒä½ç½®å¤„çš„æ•°æ®è¿›è¡Œæ±‚å’Œè¾“å‡ºä¸€ä¸ªæœ€ç»ˆçš„å€¼ï¼Œæœ€å50176çš„è¿™ä¸ªç»´åº¦ä¹Ÿå°±è¢«å‹ç¼©æ²¡äº†ç”Ÿæˆäº†æ–°çš„(32,64,3)å°ºå¯¸çš„æ•°ç»„
        return encoded_feat#(32,64,3)

    def forward(self, x):
        assert x.dim() == 4 and x.size(1) == self.in_channels
        b, in_channels, w, h = x.size()

        # [batch_size, height x width, channels]
        x = x.view(b, self.in_channels, -1).transpose(1, 2).contiguous()#(32,3,224,224)-(view)->(32,3,50176)-(transpose)->(32,50176,3)-(contiguousè¿ç»­åŒ–)->(32,50176,3)  è¿™æ ·æ“ä½œå¯ä»¥ç†è§£ä¸ºï¼Œå°†ä¸€å¼ å›¾ç‰‡ä¸­RGBå›¾åƒçš„æ‰€æœ‰åƒç´ ä¿¡æ¯æŒ‰ç…§3ä¸ªä¸€ç»„è¿›è¡Œæ’åˆ—ï¼Œç„¶ååç»­é€šè¿‡è‡ªå·±åˆ›é€ çš„ç æœ¬æ‰¾åˆ°åƒç´ ç‚¹ä¹‹é—´çš„ä¿¡æ¯ã€‚

        # assignment_weights: [batch_size, channels, num_codes]
        assignment_weights = F.softmax(self.scaled_l2(x, self.codewords, self.scale), dim=2)#è¾“å…¥ xä¸º(32,50176,3)å’Œcodewordsï¼ˆnum_codes, in_channelsï¼‰æ„æˆçš„åˆå§‹åŒ–äºŒä½å¼ é‡ï¼Œä»¥åŠscaleè¿™ä¸ªä¸€ç»´å¼ é‡

        # aggregate
        encoded_feat = self.aggregate(assignment_weights, x, self.codewords)#(32,64,3)
        return encoded_feat

